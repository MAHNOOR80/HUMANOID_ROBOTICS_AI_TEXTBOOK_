# Implementation Plan: Retrieval Pipeline Testing

**Branch**: `003-retrieval-pipeline-test` | **Date**: 2025-12-12 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/003-retrieval-pipeline-test/spec.md`

**Note**: This template is filled in by the `/sp.plan` command. See `.specify/templates/commands/plan.md` for the execution workflow.

## Summary

Implement a comprehensive testing utility (`retrieve.py`) to validate the Qdrant retrieval pipeline by testing:
1. Direct retrieval of embeddings by ID
2. Similarity search with configurable parameters (top-k, thresholds, filters)
3. End-to-end pipeline validation from stored embeddings to query results
4. Error handling for edge cases (empty collections, connection failures, invalid queries)

This testing module will serve as the validation layer for the RAG system, ensuring embeddings stored by the 001-embedding-pipeline can be successfully retrieved and semantically searched.

## Technical Context

**Language/Version**: Python 3.11
**Primary Dependencies**: qdrant-client>=1.8.0, python-dotenv>=1.0.0, cohere>=4.9.0 (for generating query embeddings)
**Storage**: Qdrant vector database (cloud instance at europe-west3-0.gcp.cloud.qdrant.io)
**Testing**: pytest>=7.4.0 for test framework, manual test scripts for validation
**Target Platform**: Cross-platform Python script (Windows/Linux/macOS compatible)
**Project Type**: Single project (Python utility module)
**Performance Goals**:
- Retrieval by ID: <100ms for collections up to 10K vectors
- Similarity search: <500ms for top-10 results in collections up to 100K vectors
- Test suite execution: Complete in <60 seconds for standard test cases
**Constraints**:
- Must use existing Qdrant connection configuration from .env
- Must be compatible with embeddings generated by 001-embedding-pipeline (1024-dimensional Cohere vectors)
- Must handle network latency to cloud Qdrant instance
**Scale/Scope**:
- Support testing collections with 1K-100K embeddings
- Validate 10+ test scenarios covering success and error paths
- Generate detailed test reports with timing and relevance metrics

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### Constitution Alignment

**I. Docusaurus-First Architecture**: ✅ PASS (Not Applicable - Backend utility, no Docusaurus content)

**II. Phase-Chapter Correspondence**: ✅ PASS (Not Applicable - Infrastructure utility, not textbook content)

**III. Content Completeness & Clarity**: ✅ PASS (Not Applicable - Code implementation, not educational content)

**IV. AI-Native Content Generation**: ✅ PASS
- Will verify Qdrant client API usage through official documentation
- Will reference existing implementation patterns from main.py and test_qdrant_connection.py
- All technical decisions grounded in verifiable sources

**V. Minimal Design, Maximum Utility**: ✅ PASS
- Single Python file (retrieve.py) for retrieval testing
- Reuses existing configuration (.env) and dependencies
- No unnecessary abstractions or over-engineering
- Clear, focused purpose: validate retrieval functionality

**VI. Hierarchical Navigation & Accessibility**: ✅ PASS (Not Applicable - Utility script, not user-facing content)

**VII. Iterative Refinement & Version Control**: ✅ PASS
- All changes tracked in Git (branch: 003-retrieval-pipeline-test)
- Implementation follows spec-driven development workflow
- Testing validates technical accuracy

**Constitution Compliance Summary**: All applicable principles pass. This is a backend utility that supports the AI-native textbook project by validating the RAG retrieval infrastructure.

## Project Structure

### Documentation (this feature)

```text
specs/003-retrieval-pipeline-test/
├── plan.md              # This file (/sp.plan command output)
├── research.md          # Phase 0 output: Technical research findings
├── data-model.md        # Phase 1 output: Entity and data structure definitions
├── quickstart.md        # Phase 1 output: Usage guide for retrieve.py
├── contracts/           # Phase 1 output: API contracts (if applicable)
└── tasks.md             # Phase 2 output (/sp.tasks command - NOT created by /sp.plan)
```

### Source Code (repository root)

```text
# Single project structure - Python utility module
retrieve.py              # Main retrieval testing utility (NEW FILE)

# Existing files (context)
main.py                  # Embedding pipeline (001-embedding-pipeline)
test_qdrant_connection.py # Basic Qdrant connection test
.env                     # Configuration (Qdrant URL, API keys)
pyproject.toml          # Dependencies

# Test structure (to be created)
tests/
├── test_retrieve.py     # Unit tests for retrieve.py
├── test_similarity.py   # Tests for similarity search
└── fixtures/            # Test data and fixtures
    ├── sample_embeddings.json
    └── query_test_cases.json
```

**Structure Decision**: Using single project structure (Option 1) because this is a standalone Python utility that integrates with the existing embedding pipeline. The retrieve.py module will be placed at the repository root alongside main.py for consistency. Test files follow pytest conventions under tests/ directory.

## Complexity Tracking

> No constitution violations requiring justification. All complexity is essential for the retrieval testing functionality.

## Phase 0: Research & Technical Discovery

### Research Questions

1. **Qdrant Query API**: What are the exact methods and parameters for:
   - Retrieving points by ID
   - Similarity search with filters
   - Batch retrieval operations
   - Error handling patterns

2. **Cohere Embedding Generation**: How to generate query embeddings compatible with stored embeddings:
   - Embedding model consistency (embed-multilingual-v3.0)
   - Input text preprocessing
   - Dimension validation

3. **Testing Patterns**: Best practices for testing vector retrieval:
   - Fixture design for known embedding-query pairs
   - Relevance validation strategies
   - Performance benchmarking approaches

4. **Error Handling**: Comprehensive error scenarios:
   - Network failures and timeouts
   - Empty collection handling
   - Dimension mismatch errors
   - Invalid filter syntax

### Research Findings (to be documented in research.md)

Topics to investigate:
- Qdrant Python client documentation (retrieve, search, scroll methods)
- Cohere API usage for query embedding generation
- Pytest fixture patterns for integration tests
- Similarity metric calculations (cosine similarity thresholds)

## Phase 1: Design Artifacts

### Entities (data-model.md)

**RetrievalQuery**:
- collection_name: str - Target Qdrant collection
- query_vector: List[float] - Query embedding (1024 dimensions)
- top_k: int - Number of results to return (default: 10)
- score_threshold: Optional[float] - Minimum similarity score filter
- filters: Optional[Dict] - Metadata filters for search

**RetrievalResult**:
- id: str - Point ID in Qdrant
- score: float - Similarity score (0.0 to 1.0 for cosine)
- vector: List[float] - Retrieved embedding vector
- metadata: Dict - Associated payload (source_url, content, chunk_id, etc.)
- retrieval_time_ms: float - Time taken to retrieve

**TestCase**:
- name: str - Test case identifier
- query_text: str - Input query text
- expected_results: List[str] - Expected document IDs or content snippets
- relevance_threshold: float - Minimum relevance score
- status: str - pass/fail/error

### API Contracts (contracts/)

**retrieve.py Public API**:

```python
# Main functions to be implemented

def connect_qdrant(url: str, api_key: str) -> QdrantClient:
    """Establish connection to Qdrant instance."""
    pass

def retrieve_by_id(client: QdrantClient, collection_name: str, point_ids: List[str]) -> List[RetrievalResult]:
    """Retrieve embeddings by their IDs."""
    pass

def similarity_search(
    client: QdrantClient,
    collection_name: str,
    query_vector: List[float],
    top_k: int = 10,
    score_threshold: Optional[float] = None,
    filters: Optional[Dict] = None
) -> List[RetrievalResult]:
    """Perform similarity search with query vector."""
    pass

def generate_query_embedding(text: str, cohere_api_key: str) -> List[float]:
    """Generate embedding for query text using Cohere."""
    pass

def validate_collection(client: QdrantClient, collection_name: str) -> Dict:
    """Validate collection exists and return metadata."""
    pass

def run_test_suite(
    client: QdrantClient,
    collection_name: str,
    test_cases: List[TestCase]
) -> Dict:
    """Execute comprehensive test suite and return results."""
    pass
```

### Quickstart Guide (quickstart.md)

Basic usage patterns:
1. Configuration via .env
2. Running basic retrieval tests
3. Executing similarity searches
4. Interpreting test results
5. Common troubleshooting scenarios

## Phase 2: Implementation Tasks (Generated by /sp.tasks)

Task generation will be handled by the `/sp.tasks` command, which will break down the implementation into atomic, testable tasks based on this plan.

## Implementation Notes

### Integration with Existing Pipeline

The retrieve.py module will:
- Reuse Qdrant connection configuration from .env (QDRANT_URL, QDRANT_API_KEY)
- Reuse Cohere API configuration from .env (COHERE_API_KEY)
- Work with collections created by main.py (001-embedding-pipeline)
- Follow existing code patterns from test_qdrant_connection.py

### Key Dependencies

All dependencies already present in pyproject.toml:
- qdrant-client>=1.8.0 (vector database client)
- cohere>=4.9.0 (embedding generation)
- python-dotenv>=1.0.0 (configuration management)
- pytest>=7.4.0 (test framework)

### Testing Strategy

1. **Unit Tests**: Test individual functions (connect, retrieve_by_id, similarity_search)
2. **Integration Tests**: Test against live Qdrant instance with test collection
3. **End-to-End Tests**: Validate full pipeline (embed → store → retrieve)
4. **Performance Tests**: Measure retrieval latency against success criteria

### Success Validation

Implementation will be validated against specification success criteria:
- SC-001: Retrieval by ID <100ms (measure with timing decorator)
- SC-002: Similarity search <500ms (benchmark with varying collection sizes)
- SC-003: 95% relevance in top-5 (validate with known query-document pairs)
- SC-004: 100% test case success (comprehensive error scenario coverage)
- SC-005: 100% error handling (test all edge cases from spec)
- SC-006: Repeatable test execution (pytest with fixtures)

## Risk Analysis

### Technical Risks

1. **Network Latency to Cloud Qdrant**: Mitigation - Include timeout handling and retry logic
2. **Embedding Dimension Mismatch**: Mitigation - Validate dimensions before queries
3. **Collection Not Found**: Mitigation - Graceful error messages with collection list
4. **API Rate Limiting (Cohere)**: Mitigation - Reuse embeddings where possible, cache test queries

### Mitigation Strategies

- Comprehensive error handling with clear error messages
- Logging for debugging (reuse logging patterns from main.py)
- Validation checks before expensive operations
- Fallback behaviors for network failures

## Next Steps

After plan approval:
1. Execute `/sp.tasks` to generate implementation tasks from this plan
2. Create research.md with detailed findings on Qdrant query API
3. Create data-model.md with entity definitions
4. Create quickstart.md with usage examples
5. Implement retrieve.py following the designed API contracts
6. Write comprehensive tests validating all success criteria
